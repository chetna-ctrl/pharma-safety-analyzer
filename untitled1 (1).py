# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11m-3OwvLzSKxb-_BEI3Aa_5R1_DMW2iW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# --- PART A: DATA GENERATION ---
def generate_antigravity_data(n_samples=1000):
    np.random.seed(42) # Result consistent rakhne ke liye
    # Base Inputs (Simulating Sensors)
    P_in = np.random.uniform(1.0, 1.1, n_samples)   # bar
    P_out = np.random.uniform(3.5, 4.5, n_samples)  # bar
    T_in = np.random.uniform(25, 30, n_samples) + 273.15 # Kelvin
    gamma = 1.4

    # 1. Physics Calculation: Ideal Discharge Temp (Isentropic)
    T_out_ideal = T_in * (P_out / P_in)**((gamma-1)/gamma)

    # 2. Introducing Degradation (Fouling/Wear)
    # 0.95 (Healthy) to 0.70 (Failing)
    eff = np.random.uniform(0.7, 0.95, n_samples)
    T_out_actual = T_out_ideal / eff

    # 3. Labeling based on Efficiency
    def get_status(e):
        if e > 0.88: return "Healthy"
        if e > 0.80: return "Warning (Energy Loss)"
        return "Critical (Carbon Intensive)"

    status = [get_status(e) for e in eff]

    # 4. Carbon & Exergy Metrics (Gouy-Stodola Theorem)
    # Lost Work (W_lost) = T0 * S_gen
    S_gen = np.log(T_out_actual / T_in) - ((gamma-1)/gamma) * np.log(P_out/P_in)
    exergy_loss = 298.15 * S_gen
    carbon_footprint = exergy_loss * 0.5

    df = pd.DataFrame({
        'P_in': P_in, 'P_out': P_out, 'T_in': T_in,
        'T_out_actual': T_out_actual, 'Isentropic_Efficiency': eff,
        'Exergy_Destruction_kJ_kg': exergy_loss,
        'Carbon_Impact_kgCO2': carbon_footprint,
        'Health_Status': status,
        'Entropy_Generation': S_gen
    })

    # Save for Step 2
    df.to_csv('antigravity_step1_dataset.csv', index=False)
    return df

# --- RUNNING THE ENGINE ---
print("‚öôÔ∏è Generating Physics-Consistent Data...")
df = generate_antigravity_data()
print("‚úÖ Step 1: Dataset Created & Saved as 'antigravity_step1_dataset.csv'")

# --- PART B: VISUALIZATION ---
print("\nüìä Launching Physics Insights...")

# 1. Efficiency vs. Carbon Impact
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Isentropic_Efficiency', y='Carbon_Impact_kgCO2', hue='Health_Status', palette='viridis')
plt.title('AntiGravity Insight: Thermodynamic Efficiency vs Carbon Footprint')
plt.grid(True, alpha=0.3)
plt.show()

# 2. Gouy-Stodola Audit (Interactive)
fig = px.scatter(df, x="Entropy_Generation", y="Exergy_Destruction_kJ_kg",
                 color="Health_Status", size="Carbon_Impact_kgCO2",
                 title="AntiGravity Audit: Gouy-Stodola Theorem Verification",
                 labels={"Entropy_Generation": "Entropy Generated (S_gen)",
                         "Exergy_Destruction_kJ_kg": "Useful Work Lost (W_lost)"})
fig.show()

# --- PART C: DATA AUDIT SUMMARY ---
print("\n--- üõ°Ô∏è AntiGravity Data Audit Summary ---")
summary = df.groupby('Health_Status')[['Carbon_Impact_kgCO2', 'Exergy_Destruction_kJ_kg']].mean()
print(summary)

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the Physics-Consistent Dataset
df = pd.read_csv('antigravity_step1_dataset.csv')

# 2. Feature Selection (Surgical Choice)
# Humne wahi features liye hain jo thermodynamics aur carbon footprint ko define karte hain
features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation']
X = df[features]

# Target: Health_Status (Healthy, Warning, Critical)
# Label Encoding: Healthy=0, Warning=1, Critical=2
target_map = {"Healthy": 0, "Warning (Energy Loss)": 1, "Critical (Carbon Intensive)": 2}
y = df['Health_Status'].map(target_map)

# 3. Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 4. Train XGBoost Model
print("üöÄ Training AntiGravity Physics-Informed Model...")
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    objective='multi:softprob',
    random_state=42
)
model.fit(X_train, y_train)

# 5. Model Evaluation
y_pred = model.predict(X_test)
print("\n--- üõ°Ô∏è Model Performance Report ---")
print(classification_report(y_test, y_pred, target_names=target_map.keys()))

# 6. Feature Importance Visualization
# Ye dikhayega ki model 'Isentropic Efficiency' ko kitni priority de raha hai
plt.figure(figsize=(10, 6))
xgb.plot_importance(model, importance_type='weight', color='royalblue')
plt.title('AntiGravity Logic: Feature Importance (Physics Weights)')
plt.show()

# Save model for Dashboard
import joblib
joblib.dump(model, 'antigravity_model_v1.pkl')
print("\n‚úÖ Step 2: Model Trained & Saved as 'antigravity_model_v1.pkl'")

import streamlit as st
import pandas as pd
import numpy as np
import joblib

# Load Model from Step 2
model = joblib.load('antigravity_model_v1.pkl')

st.set_page_config(page_title="AntiGravity Sustainability Dashboard", layout="wide")

st.title("üõ°Ô∏è AntiGravity: Physics-Informed ESG Platform")
st.markdown("### Powered by Thermodynamics & Carbon-Aware AI")

# Sidebar: Sensor Inputs (The "Digital Twin" Controls)
st.sidebar.header("Industrial Sensor Inputs")
p_in = st.sidebar.slider("Suction Pressure (bar)", 1.0, 1.2, 1.05)
p_out = st.sidebar.slider("Discharge Pressure (bar)", 3.0, 6.0, 4.0)
t_in = st.sidebar.slider("Suction Temp (K)", 290.0, 310.0, 298.15)
t_out_act = st.sidebar.slider("Actual Discharge Temp (K)", 350.0, 700.0, 450.0)

# Thermodynamic Calculation Logic (The "Engineer's Brain")
gamma = 1.4
cp = 1.005
r = 0.287
t0 = 298.15

t_ideal = t_in * (p_out / p_in)**((gamma-1)/gamma)
eff = t_ideal / t_out_act
s_gen = cp * np.log(t_out_act / t_in) - r * np.log(p_out / p_in)
exergy_loss = t0 * s_gen
carbon_impact = exergy_loss * 0.138 # kgCO2/kJ

# Logic for Model Prediction
input_data = pd.DataFrame([[p_in, p_out, t_in, t_out_act, eff, s_gen]],
                          columns=['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation'])
prediction = model.predict(input_data)[0]
status_map = {0: "Healthy", 1: "Warning (Energy Loss)", 2: "Critical (Carbon Risk)"}

# Dashboard Layout
col1, col2, col3 = st.columns(3)

with col1:
    st.metric("Sustainability Status", status_map[prediction])
    st.progress(eff if eff <= 1 else 1)
    st.write(f"Thermodynamic Efficiency: {eff*100:.2f}%")

with col2:
    st.metric("Carbon Footprint Impact", f"{carbon_impact:.2f} kgCO2/kg", delta=f"{carbon_impact-0.5:.2f}", delta_color="inverse")
    st.write("Metric: Emissions Intensity (IPCC Standard)")

with col3:
    penalty = exergy_loss * 15 # Assuming ‚Çπ15 per kJ of waste energy
    st.metric("Hourly Energy Penalty", f"‚Çπ{penalty:.2f}")
    st.write("Financial Risk of Inefficiency")

st.divider()

# XAI & Recommendation
st.subheader("üí° Engineering Recommendation")
if prediction == 2:
    st.error("ACTION REQUIRED: High Exergy Destruction detected. Inspect valves and lubrication to reduce Carbon Penalty.")
elif prediction == 1:
    st.warning("OPTIMIZATION NEEDED: Isentropic efficiency drifting. Schedule filter cleaning to save on Energy Costs.")
else:
    st.success("SYSTEM STABLE: Machine is operating within thermodynamic safety bounds.")

st.info("This AI prediction is validated by Gouy-Stodola Theorem (Physics-Constrained).")

!pip install streamlit joblib xgboost shap plotly

# Colab par zaroori libraries install karne ke liye
!pip install xgboost shap scikit-learn pandas matplotlib seaborn

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# 1. Load Dataset (Jo humne Step 1 mein banaya tha)
try:
    df = pd.read_csv('antigravity_step1_dataset.csv')
    print("‚úÖ Dataset loaded successfully!")
except:
    print("‚ùå Error: 'antigravity_step1_dataset.csv' nahi mili. Pehle Step 1 run kijiye.")

# 2. Features and Target Selection
# Humne wahi features liye jo thermodynamics ko define karte hain
X = df[['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation']]
y = df['Health_Status']

# Label Encoding for the Target (Healthy, Warning, Critical)
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# 3. Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# 4. Model Tuning & Optimization (Grid Search)
print("‚öôÔ∏è Tuning Hyperparameters (Optimization in progress)...")
param_grid = {
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 150],
    'subsample': [0.8, 1.0]
}

base_xgb = xgb.XGBClassifier(objective='multi:softprob', random_state=42)
grid_search = GridSearchCV(estimator=base_xgb, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)
grid_search.fit(X_train, y_train)

# 5. Best Model Evaluation
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Model Performance Percentage
accuracy = accuracy_score(y_test, y_pred) * 100
print(f"\nüéØ Model Trained: {accuracy:.2f}% Accuracy (Performance)")
print(f"üîù Best Parameters: {grid_search.best_params_}")

# 6. Detailed Performance Report
print("\n--- üõ°Ô∏è Classification Report ---")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 7. Visualization: Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('AntiGravity Validation: Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# 8. Physics Priority Check (Feature Importance)
plt.figure(figsize=(10, 6))
feat_importances = pd.Series(best_model.feature_importances_, index=X.columns)
feat_importances.nlargest(6).plot(kind='barh', color='teal')
plt.title('Physics Sensitivity Analysis (Model Weightage)')
plt.show()

# Save Optimized Model
joblib.dump(best_model, 'antigravity_optimized_model.pkl')
joblib.dump(le, 'label_encoder.pkl')
print("‚úÖ Optimized Model saved as 'antigravity_optimized_model.pkl'")

import pandas as pd
import joblib

# 1. Load Model and Encoder
model = joblib.load('antigravity_optimized_model.pkl')
le = joblib.load('label_encoder.pkl')

# 2. Define Test Cases (Industrial Scenarios)
test_scenarios = {
    "Case A: Healthy Operations": {
        'P_in': 1.05, 'P_out': 3.5, 'T_in': 298.15,
        'T_out_actual': 430.0, 'Isentropic_Efficiency': 0.94, 'Entropy_Generation': 0.05
    },
    "Case B: Thermodynamic Inefficiency (Carbon Risk)": {
        'P_in': 1.02, 'P_out': 4.0, 'T_in': 300.0,
        'T_out_actual': 620.0, 'Isentropic_Efficiency': 0.65, 'Entropy_Generation': 0.42
    },
    "Case C: Early Warning (Fouling/Wear)": {
        'P_in': 1.1, 'P_out': 4.2, 'T_in': 295.0,
        'T_out_actual': 510.0, 'Isentropic_Efficiency': 0.79, 'Entropy_Generation': 0.18
    }
}

# 3. Execution & Validation
print("üõ°Ô∏è Starting AntiGravity Physics Validation...")
print("-" * 50)

for name, inputs in test_scenarios.items():
    # Convert to DataFrame for prediction
    test_df = pd.DataFrame([inputs])

    # Predict
    pred_encoded = model.predict(test_df)[0]
    pred_label = le.inverse_transform([pred_encoded])[0]

    # Probabilities (Confidence Check)
    probs = model.predict_proba(test_df)[0]
    confidence = max(probs) * 100

    print(f"üìå {name}")
    print(f"   AI Prediction: {pred_label}")
    print(f"   Confidence: {confidence:.2f}%")

    # Physics Audit
    if inputs['Entropy_Generation'] > 0.3:
        print(f"   Audit Check: High Entropy Detected. Physics matches Carbon Risk.")
    elif inputs['Isentropic_Efficiency'] > 0.9:
        print(f"   Audit Check: Low Irreversibility. Physics matches Healthy State.")
    print("-" * 50)

import pandas as pd
import joblib
import numpy as np

# 1. Load optimized model and encoder
model = joblib.load('antigravity_optimized_model.pkl')
le = joblib.load('label_encoder.pkl')
df_full = pd.read_csv('antigravity_step1_dataset.csv')

# --- PART A: 10 TEST CASES (WITHIN DATASET) ---
print("üìä [PART A] Testing 10 Samples from Dataset (Validation)...")
test_within = df_full.sample(10, random_state=7)
X_within = test_within[['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation']]
y_actual = test_within['Health_Status']

preds_within = le.inverse_transform(model.predict(X_within))

results_within = pd.DataFrame({
    'Actual_Status': y_actual.values,
    'AI_Prediction': preds_within,
    'Correct?': y_actual.values == preds_within
})
print(results_within)
print("-" * 50)

# --- PART B: 10 TEST CASES (OUTSIDE DATASET - MANUAL SCENARIOS) ---
print("üöÄ [PART B] Testing 10 NEW Scenarios (Physics Stress Test)...")

# Designing 10 unique scenarios that were NOT in the original random CSV
manual_data = [
    # Healthy Cases (High Efficiency)
    {'P_in': 1.05, 'P_out': 3.2, 'T_in': 298, 'T_out_actual': 415, 'Isentropic_Efficiency': 0.95, 'Entropy_Generation': 0.04}, # Case 1
    {'P_in': 1.08, 'P_out': 3.8, 'T_in': 295, 'T_out_actual': 430, 'Isentropic_Efficiency': 0.92, 'Entropy_Generation': 0.06}, # Case 2

    # Warning Cases (Degrading / Early Fouling)
    {'P_in': 1.02, 'P_out': 4.5, 'T_in': 305, 'T_out_actual': 560, 'Isentropic_Efficiency': 0.82, 'Entropy_Generation': 0.22}, # Case 3
    {'P_in': 1.04, 'P_out': 3.9, 'T_in': 300, 'T_out_actual': 490, 'Isentropic_Efficiency': 0.84, 'Entropy_Generation': 0.19}, # Case 4
    {'P_in': 1.07, 'P_out': 4.1, 'T_in': 297, 'T_out_actual': 515, 'Isentropic_Efficiency': 0.81, 'Entropy_Generation': 0.21}, # Case 5

    # Critical Cases (Carbon Intensive / High Irreversibility)
    {'P_in': 1.01, 'P_out': 5.0, 'T_in': 310, 'T_out_actual': 720, 'Isentropic_Efficiency': 0.62, 'Entropy_Generation': 0.48}, # Case 6
    {'P_in': 1.03, 'P_out': 4.8, 'T_in': 308, 'T_out_actual': 700, 'Isentropic_Efficiency': 0.64, 'Entropy_Generation': 0.45}, # Case 7
    {'P_in': 1.05, 'P_out': 5.5, 'T_in': 303, 'T_out_actual': 750, 'Isentropic_Efficiency': 0.58, 'Entropy_Generation': 0.55}, # Case 8

    # Extreme Edge Cases (The "Stress" test)
    {'P_in': 1.10, 'P_out': 6.0, 'T_in': 298, 'T_out_actual': 800, 'Isentropic_Efficiency': 0.55, 'Entropy_Generation': 0.65}, # Case 9: Ultra high pressure/temp
    {'P_in': 1.00, 'P_out': 3.0, 'T_in': 315, 'T_out_actual': 550, 'Isentropic_Efficiency': 0.72, 'Entropy_Generation': 0.32}  # Case 10: High suction temp
]

X_outside = pd.DataFrame(manual_data)
preds_outside = le.inverse_transform(model.predict(X_outside))
probs_outside = model.predict_proba(X_outside).max(axis=1) * 100

results_outside = pd.DataFrame({
    'Scenario': [f"New Case {i+1}" for i in range(10)],
    'AI_Prediction': preds_outside,
    'Confidence %': probs_outside,
    'Entropy_Level': X_outside['Entropy_Generation']
})
print(results_outside)

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib

# 1. Load Original Data
df = pd.read_csv('antigravity_step1_dataset.csv')

# 2. Features and Target
X = df[['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation']]
le = LabelEncoder()
y_encoded = le.fit_transform(df['Health_Status'])

# 3. Create NOISY DATA (Industrial Realism)
# Hum 2% random variation add kar rahe hain sensors mein
X_noisy = X.copy()
X_noisy['Entropy_Generation'] += np.random.normal(0, 0.015, len(X))
X_noisy['Isentropic_Efficiency'] += np.random.normal(0, 0.01, len(X))

# 4. Train-Test Split with Noisy Data
X_train_noisy, X_test_noisy, y_train_noisy, y_test_noisy = train_test_split(
    X_noisy, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# 5. Train Model (Making it robust)
print("‚öôÔ∏è Re-training AntiGravity with Industrial Noise...")
model_noisy = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.05, # Learning rate thoda kam kiya for stability
    max_depth=4,
    objective='multi:softprob',
    random_state=42
)
model_noisy.fit(X_train_noisy, y_train_noisy)

# 6. Save this robust model
joblib.dump(model_noisy, 'antigravity_optimized_model.pkl')
print("‚úÖ Optimized & Robust Model Saved!")

# 7. Check confidence on a test sample
sample_test = X_test_noisy.iloc[[0]]
prob = model_noisy.predict_proba(sample_test).max() * 100
print(f"üìä New Average Confidence on Unseen Data: {prob:.2f}%")

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib

# 1. Load Data
df = pd.read_csv('antigravity_step1_dataset.csv')

# 2. Features and Target
X = df[['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation']]
le = LabelEncoder()
y_encoded = le.fit_transform(df['Health_Status'])

# 3. Controlled Noise (Noise ko thoda kam karte hain for better stability)
X_boosted = X.copy()
X_boosted['Entropy_Generation'] += np.random.normal(0, 0.005, len(X)) # Noise reduced from 0.015 to 0.005
X_boosted['Isentropic_Efficiency'] += np.random.normal(0, 0.002, len(X))

# 4. Split
X_train, X_test, y_train, y_test = train_test_split(X_boosted, y_encoded, test_size=0.2, random_state=42)

# 5. HIGH-PERFORMANCE Training (Boosting Settings)
print("üî• Boosting AntiGravity Engine Performance...")
model_boosted = xgb.XGBClassifier(
    n_estimators=500,        # Increased from 100
    learning_rate=0.01,      # Slower learning for better precision
    max_depth=8,             # Deeper trees to capture physics nuances
    subsample=0.9,           # Randomly sample data to avoid overfitting
    colsample_bytree=0.9,
    objective='multi:softprob',
    random_state=42
)

model_boosted.fit(X_train, y_train)

# 6. Save the Boosted Model
joblib.dump(model_boosted, 'antigravity_optimized_model.pkl')
print("‚úÖ Boosted Model Saved!")

# 7. Final Confidence Check
sample_test = X_test.iloc[[0]]
prob = model_boosted.predict_proba(sample_test).max() * 100
print(f"üéØ NEW Boosted Confidence: {prob:.2f}%")

# --- STEP 2.4: Regularization for Natural Confidence ---
print("‚öñÔ∏è Balancing AntiGravity Engine (Adding Regularization)...")

model_natural = xgb.XGBClassifier(
    n_estimators=200,
    learning_rate=0.03,
    max_depth=5,            # Depth kam ki taaki overfit na ho
    reg_alpha=10,           # L1 Regularization (Complexity ko punish karega)
    reg_lambda=10,          # L2 Regularization (Weights ko small rakhega)
    objective='multi:softprob',
    random_state=42
)

model_natural.fit(X_train, y_train)

# Final Check
sample_test = X_test.iloc[[0]]
prob = model_natural.predict_proba(sample_test).max() * 100
print(f"üéØ NATURAL Balanced Confidence: {prob:.2f}%")

import pandas as pd
import joblib

# 1. Load the Natural/Balanced Model
model = joblib.load('antigravity_optimized_model.pkl')
le = joblib.load('label_encoder.pkl')
df_full = pd.read_csv('antigravity_step1_dataset.csv')

# --- PART A: 10 SAMPLES FROM DATASET ---
print("üìä [PART A] Validating with Dataset Samples...")
test_within = df_full.sample(10, random_state=42)
X_within = test_within[['P_in', 'P_out', 'T_in', 'T_out_actual', 'Isentropic_Efficiency', 'Entropy_Generation']]
y_actual = test_within['Health_Status']

preds_within = le.inverse_transform(model.predict(X_within))
print(pd.DataFrame({'Actual': y_actual.values, 'AI_Pred': preds_within, 'Match': y_actual.values == preds_within}))

print("\n" + "="*50 + "\n")

# --- PART B: 10 NEW UNSEEN SCENARIOS ---
print("üöÄ [PART B] Stress Testing with UNSEEN Physics Scenarios...")

manual_scenarios = [
    # 1-2: Healthy
    {'P_in': 1.05, 'P_out': 3.2, 'T_in': 298, 'T_out_actual': 415, 'Isentropic_Efficiency': 0.95, 'Entropy_Generation': 0.04},
    {'P_in': 1.08, 'P_out': 3.8, 'T_in': 295, 'T_out_actual': 430, 'Isentropic_Efficiency': 0.92, 'Entropy_Generation': 0.06},
    # 3-5: Warning
    {'P_in': 1.02, 'P_out': 4.5, 'T_in': 305, 'T_out_actual': 560, 'Isentropic_Efficiency': 0.82, 'Entropy_Generation': 0.22},
    {'P_in': 1.04, 'P_out': 3.9, 'T_in': 300, 'T_out_actual': 490, 'Isentropic_Efficiency': 0.84, 'Entropy_Generation': 0.19},
    {'P_in': 1.07, 'P_out': 4.1, 'T_in': 297, 'T_out_actual': 515, 'Isentropic_Efficiency': 0.81, 'Entropy_Generation': 0.21},
    # 6-10: Critical
    {'P_in': 1.01, 'P_out': 5.0, 'T_in': 310, 'T_out_actual': 720, 'Isentropic_Efficiency': 0.62, 'Entropy_Generation': 0.48},
    {'P_in': 1.03, 'P_out': 4.8, 'T_in': 308, 'T_out_actual': 700, 'Isentropic_Efficiency': 0.64, 'Entropy_Generation': 0.45},
    {'P_in': 1.10, 'P_out': 6.0, 'T_in': 298, 'T_out_actual': 800, 'Isentropic_Efficiency': 0.55, 'Entropy_Generation': 0.65},
    {'P_in': 1.00, 'P_out': 3.0, 'T_in': 315, 'T_out_actual': 550, 'Isentropic_Efficiency': 0.72, 'Entropy_Generation': 0.32},
    {'P_in': 1.05, 'P_out': 5.5, 'T_in': 303, 'T_out_actual': 750, 'Isentropic_Efficiency': 0.58, 'Entropy_Generation': 0.55}
]

X_outside = pd.DataFrame(manual_scenarios)
probs = model.predict_proba(X_outside)
preds_outside = le.inverse_transform(model.predict(X_outside))

results_outside = pd.DataFrame({
    'Case': [f"Scenario {i+1}" for i in range(10)],
    'AI_Prediction': preds_outside,
    'Confidence%': probs.max(axis=1) * 100,
    'S_gen': X_outside['Entropy_Generation']
})
print(results_outside)

import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
from sklearn.preprocessing import LabelEncoder

# --- STEP 1: ADVANCED DATA GENERATION (With Real Gas Logic) ---
def generate_industrial_data(n_samples=2000):
    np.random.seed(42)
    P_in = np.random.uniform(1.0, 1.1, n_samples)
    P_ratio = np.random.uniform(3.5, 5.0, n_samples)
    P_out = P_in * P_ratio
    T_in = np.random.uniform(25, 30, n_samples) + 273.15
    gamma = 1.4

    # 1. Real Gas Deviation (Peng-Robinson Simplified Z-factor)
    # Equation: Z = f(P, T) - Industrial gases deviate at high P
    Z_factor = 1 - (P_out * 0.004)

    # 2. Advanced Efficiency Equation
    # Actual Work includes Z-factor for Real Gases
    T_out_ideal = T_in * (P_ratio)**((gamma-1)/gamma)
    eff = np.random.uniform(0.7, 0.95, n_samples)
    T_out_actual = T_out_ideal / (eff * Z_factor) # Corrected for Real Gas

    # 3. Entropy & Exergy (Gouy-Stodola Theorem)
    S_gen = np.log(T_out_actual / T_in) - ((gamma-1)/gamma) * np.log(P_ratio)
    # Net Entropy after considering Inter-stage Cooling
    cooling_effect = np.where(T_out_actual > 550, 0.04, 0.01)
    net_s_gen = S_gen - cooling_effect

    status = []
    for e in eff:
        if e > 0.88: status.append("Healthy")
        elif e > 0.80: status.append("Warning (Energy Loss)")
        else: status.append("Critical (Carbon Intensive)")

    return pd.DataFrame({
        'P_in': P_in, 'P_out': P_out, 'T_in': T_in,
        'T_out_actual': T_out_actual, 'Corrected_Efficiency': eff * Z_factor,
        'Net_S_gen': net_s_gen, 'Health_Status': status
    })

print("‚öôÔ∏è Generating Industrial Dataset with Solution Thermodynamics...")
df_industrial = generate_industrial_data()

# --- STEP 2: INDUSTRIAL MODEL TRAINING ---
features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Corrected_Efficiency', 'Net_S_gen']
X = df_industrial[features]

le = LabelEncoder()
y = le.fit_transform(df_industrial['Health_Status'])

print("üß™ Training Advanced AntiGravity Engine...")
industrial_model = xgb.XGBClassifier(
    n_estimators=300,
    learning_rate=0.03,
    max_depth=6,
    reg_lambda=15,
    objective='multi:softprob',
    random_state=42
)
industrial_model.fit(X, y)

# Save everything
joblib.dump(industrial_model, 'antigravity_industrial_model.pkl')
joblib.dump(le, 'label_encoder.pkl')
print("‚úÖ SUCCESS: Industrial-Grade Model Trained & Saved!")

import pandas as pd
import joblib
import numpy as np

# 1. Load Advanced Model & Encoder
model = joblib.load('antigravity_industrial_model.pkl')
le = joblib.load('label_encoder.pkl')

# --- PART A: 10 TEST CASES (WITHIN DATASET) ---
# Hum industrial dataframe se random samples uthayenge
test_within = df_industrial.sample(10, random_state=42)
X_within = test_within[['P_in', 'P_out', 'T_in', 'T_out_actual', 'Corrected_Efficiency', 'Net_S_gen']]
y_actual = test_within['Health_Status']

preds_within = le.inverse_transform(model.predict(X_within))
print("üìä [PART A] Validation (Within Dataset):")
print(pd.DataFrame({'Actual': y_actual.values, 'AI_Pred': preds_within, 'Match': y_actual.values == preds_within}))

print("\n" + "="*60 + "\n")

# --- PART B: 10 TEST CASES (OUTSIDE DATASET - REAL GAS SCENARIOS) ---
print("üöÄ [PART B] Stress Test (Outside Dataset - Advanced Scenarios):")

# Designing 10 unique industrial scenarios
manual_data = [
    # 1-3: Healthy (Low Compression, Good Cooling)
    {'P_in': 1.05, 'P_out': 3.5, 'T_in': 298, 'T_out_actual': 420, 'Corrected_Efficiency': 0.91, 'Net_S_gen': 0.03},
    {'P_in': 1.02, 'P_out': 3.2, 'T_in': 295, 'T_out_actual': 410, 'Corrected_Efficiency': 0.93, 'Net_S_gen': 0.02},
    {'P_in': 1.07, 'P_out': 3.6, 'T_in': 300, 'T_out_actual': 425, 'Corrected_Efficiency': 0.90, 'Net_S_gen': 0.04},

    # 4-6: Warning (Thermal Drift / Fouling)
    {'P_in': 1.03, 'P_out': 4.5, 'T_in': 302, 'T_out_actual': 530, 'Corrected_Efficiency': 0.81, 'Net_S_gen': 0.18},
    {'P_in': 1.06, 'P_out': 4.2, 'T_in': 297, 'T_out_actual': 490, 'Corrected_Efficiency': 0.83, 'Net_S_gen': 0.15},
    {'P_in': 1.01, 'P_out': 4.0, 'T_in': 305, 'T_out_actual': 510, 'Corrected_Efficiency': 0.79, 'Net_S_gen': 0.20},

    # 7-10: Critical (Real Gas Deviation + High Wear)
    {'P_in': 1.10, 'P_out': 5.5, 'T_in': 298, 'T_out_actual': 750, 'Corrected_Efficiency': 0.60, 'Net_S_gen': 0.52}, # High P ratio
    {'P_in': 1.04, 'P_out': 5.2, 'T_in': 308, 'T_out_actual': 720, 'Corrected_Efficiency': 0.62, 'Net_S_gen': 0.48}, # High T_in
    {'P_in': 1.09, 'P_out': 5.8, 'T_in': 303, 'T_out_actual': 780, 'Corrected_Efficiency': 0.55, 'Net_S_gen': 0.58}, # Max Irreversibility
    {'P_in': 1.00, 'P_out': 4.8, 'T_in': 310, 'T_out_actual': 690, 'Corrected_Efficiency': 0.64, 'Net_S_gen': 0.41}  # Transition zone
]

X_outside = pd.DataFrame(manual_data)
preds_outside = le.inverse_transform(model.predict(X_outside))
probs_outside = model.predict_proba(X_outside).max(axis=1) * 100

results_outside = pd.DataFrame({
    'Scenario': [f"Advanced Case {i+1}" for i in range(10)],
    'AI_Prediction': preds_outside,
    'Confidence %': probs_outside,
    'Efficiency': X_outside['Corrected_Efficiency']
})
print(results_outside)

import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# --- 1. DATA GENERATION WITH ADVANCED COLUMNS ---
def generate_master_dataset(n_samples=3000):
    np.random.seed(42)

    # Basic Sensors
    P_in = np.random.uniform(1.0, 1.2, n_samples)
    P_ratio = np.random.uniform(3.0, 6.0, n_samples)
    P_out = P_in * P_ratio
    T_in = np.random.uniform(290, 310, n_samples)

    # A. Z-Factor (Real Gas Deviation)
    # Z decreases as Pressure increases (Simple Peng-Robinson Logic)
    Z_factor = 1 - (P_out * 0.006)

    # B. Mass Flow Rate (kg/s)
    mass_flow = np.random.uniform(10, 100, n_samples)

    # C. Advanced Efficiency & Actual Temp
    gamma = 1.4
    cp = 1.005 # kJ/kg-K
    T_out_ideal = T_in * (P_ratio)**((gamma-1)/gamma)
    eff = np.random.uniform(0.6, 0.95, n_samples)

    # T_out_actual corrected for Efficiency and Z-factor
    T_out_actual = T_out_ideal / (eff * Z_factor)

    # D. Entropy & Cooling Recovery
    # S_gen (kJ/kg-K)
    S_gen = cp * np.log(T_out_actual/T_in) - 0.287 * np.log(P_ratio)
    # Cooling Effect (Inter-stage heat removal logic)
    cooling_recovery = np.where(T_out_actual > 500, S_gen * 0.1, S_gen * 0.02)
    Net_S_gen = S_gen - cooling_recovery

    # E. Carbon Footprint (ESG Metric)
    # Exergy Loss (To = 298.15K)
    W_lost = 298.15 * Net_S_gen
    total_carbon = mass_flow * W_lost * 0.138 # kg CO2

    # F. Health Status Labeling
    status = []
    for e in eff:
        if e > 0.88: status.append("Healthy")
        elif e > 0.78: status.append("Warning (Energy Loss)")
        else: status.append("Critical (Carbon Intensive)")

    return pd.DataFrame({
        'P_in': P_in, 'P_out': P_out, 'T_in': T_in, 'T_out_actual': T_out_actual,
        'Z_factor': Z_factor, 'Mass_Flow': mass_flow, 'Net_S_gen': Net_S_gen,
        'Efficiency': eff, 'Total_Carbon': total_carbon, 'Status': status
    })

# --- 2. EXECUTION ---
print("üè≠ Creating Master Industrial Dataset...")
df_master = generate_master_dataset()

# Select Features for AI (Removing 'Status' and target-leaking 'Efficiency')
features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Z_factor', 'Mass_Flow', 'Net_S_gen']
X = df_master[features]

le = LabelEncoder()
y = le.fit_transform(df_master['Status'])

# --- 3. MODEL TRAINING ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("üß† Training AntiGravity Industrial Model...")
final_model = xgb.XGBClassifier(
    n_estimators=300,
    learning_rate=0.03,
    max_depth=6,
    reg_lambda=15,
    objective='multi:softprob'
)
final_model.fit(X_train, y_train)

# Save for Dashboard
joblib.dump(final_model, 'antigravity_industrial_model.pkl')
joblib.dump(le, 'label_encoder.pkl')
print("‚úÖ SUCCESS: Model is now Industrial-Grade!")

# Confidence Check
test_sample = X_test.iloc[[0]]
print(f"üéØ Final Confidence on Industrial Data: {final_model.predict_proba(test_sample).max()*100:.2f}%")

import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# --- 1. ADVANCED DATA GENERATION (The Digital Twin) ---
def generate_v2_dataset(n_samples=5000):
    np.random.seed(42)

    # Time & Ambient Factor (Simulation of 1 year)
    days = np.linspace(0, 365, n_samples)
    ambient_temp = 298.15 + 10 * np.sin(2 * np.pi * days / 365) # Seasonal fluctuation

    # Inputs
    P_in = np.random.uniform(1.0, 1.2, n_samples)
    P_ratio = np.random.uniform(3.0, 6.0, n_samples)
    P_out = P_in * P_ratio
    T_in = ambient_temp + np.random.normal(0, 1, n_samples) # Ambient dependency

    # Mass Flow (kg/s)
    mass_flow = np.random.uniform(20, 80, n_samples)

    # A. Z-Factor (Compressibility)
    Z_factor = 1 - (P_out * 0.0055)

    # B. Polytropic & Isentropic Logic
    gamma = 1.4
    cp = 1.005 # kJ/kg-K
    n_poly = 1.5 # Polytropic index for real machines

    # Base Efficiency with Time-based Degradation (Fouling)
    # Efficiency drops slowly over 365 days
    degradation = 0.05 * (days / 365)
    base_eff = np.random.uniform(0.85, 0.95, n_samples) - degradation

    # C. Exergy Destruction (Gouy-Stodola Theorem)
    T_out_actual = T_in * (P_ratio)**((n_poly-1)/n_poly) / base_eff
    S_gen = cp * np.log(T_out_actual/T_in) - 0.287 * np.log(P_ratio)
    exergy_destruction = 298.15 * S_gen * mass_flow # kJ/s (kW)

    # D. Carbon Footprint (ESG Reporting)
    total_carbon = exergy_destruction * 0.000138 * 3600 # kg CO2 per hour

    # Status Labeling (Based on Degradation & Exergy)
    status = []
    for d in degradation:
        if d < 0.015: status.append("Healthy")
        elif d < 0.035: status.append("Warning (Degrading)")
        else: status.append("Critical (Maintenance Required)")

    return pd.DataFrame({
        'P_in': P_in, 'P_out': P_out, 'T_in': T_in, 'T_out_actual': T_out_actual,
        'Z_factor': Z_factor, 'Mass_Flow': mass_flow, 'Exergy_Destruction': exergy_destruction,
        'S_gen': S_gen, 'Total_Carbon_hr': total_carbon, 'Status': status
    })

print("üèóÔ∏è Building Master Dataset v2.0...")
df_v2 = generate_v2_dataset()

# --- 2. UNCERTAINTY-AWARE ML TRAINING ---
features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Z_factor', 'Mass_Flow', 'S_gen']
X = df_v2[features]
le = LabelEncoder()
y = le.fit_transform(df_v2['Status'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("üß† Training AntiGravity v2 (Quantile-Calibrated)...")
# Using Multi-class logloss for stable probabilities
model_v2 = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.02,
    max_depth=7,
    reg_alpha=5,
    reg_lambda=10,
    subsample=0.8,
    objective='multi:softprob'
)
model_v2.fit(X_train, y_train)

# Save Optimized Components
joblib.dump(model_v2, 'antigravity_v2_model.pkl')
joblib.dump(le, 'label_encoder_v2.pkl')
print("‚úÖ Master Engine v2.0 Ready!")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

def generate_industrial_dataset(n_samples=2000):
    np.random.seed(42)
    # 1. Raw Sensor Inputs
    P_in = np.random.uniform(1.0, 1.2, n_samples)    # bar (Suction)
    P_out = np.random.uniform(3.5, 5.5, n_samples)   # bar (Discharge)
    T_in = np.random.uniform(20, 35, n_samples) + 273.15 # Kelvin
    gamma = 1.4
    Cp = 1.005 # kJ/kg.K
    R = 0.287  # kJ/kg.K

    # 2. Physics Logic: Ideal Isentropic State
    T_out_ideal = T_in * (P_out / P_in)**((gamma-1)/gamma)

    # 3. Simulate Equipment Degradation (Hidden Efficiency)
    # We use this to label the data, but we WON'T give it to the ML model later.
    hidden_eff = np.random.uniform(0.65, 0.96, n_samples)
    T_out_actual = T_out_ideal / hidden_eff

    # 4. Thermodynamic Audit Metrics
    # Entropy Generation (S_gen) = Cp*ln(T2/T1) - R*ln(P2/P1)
    S_gen = Cp * np.log(T_out_actual / T_in) - R * np.log(P_out / P_in)
    exergy_destruction = 298.15 * S_gen # T0 * S_gen (kJ/kg)
    carbon_footprint = exergy_destruction * 0.45 # Emission factor

    # 5. Engineering Labels
    def assign_status(e):
        if e > 0.88: return "Healthy"
        elif e > 0.78: return "Warning (Energy Loss)"
        else: return "Critical (Carbon Risk)"

    df = pd.DataFrame({
        'Suction_P': P_in, 'Discharge_P': P_out,
        'Suction_T': T_in, 'Actual_Discharge_T': T_out_actual,
        'Entropy_Gen': S_gen, 'Exergy_Loss': exergy_destruction,
        'Carbon_Impact': carbon_footprint,
        'Efficiency_Label': hidden_eff, # Hidden from Model
        'Status': [assign_status(e) for e in hidden_eff]
    })

    return df

df_industrial = generate_industrial_dataset()
print("‚úÖ Dataset Generated with Thermodynamic Constraints.")

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Feature Selection (Removing the target-leaking Efficiency)
X = df_industrial[['Suction_P', 'Discharge_P', 'Suction_T', 'Actual_Discharge_T', 'Entropy_Gen']]
le = LabelEncoder()
y = le.fit_transform(df_industrial['Status'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train XGBoost
model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5)
model.fit(X_train, y_train)

# Evaluation
y_pred = model.predict(X_test)
print("\n--- üõ°Ô∏è Auditor Performance Report ---")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 1. Physics Priority Check
plt.figure(figsize=(10, 5))
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(5).plot(kind='barh', color='teal')
plt.title("Physics Consistency: Feature Importance (Entropy is Key)")
plt.show()

# 2. Gouy-Stodola Audit Plot
fig = px.scatter(df_industrial, x="Entropy_Gen", y="Exergy_Loss",
                 color="Status", size="Carbon_Impact",
                 title="Thermodynamic Audit: S_gen vs Lost Work",
                 labels={"Entropy_Gen": "Entropy Generation (kJ/kg.K)", "Exergy_Loss": "Useful Work Lost (kJ/kg)"})
fig.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import PartialDependenceDisplay

# 1. NOVELTY CHECK: Sensitivity Analysis
# Hum check karenge ki agar hum sirf Discharge Temperature badhayein,
# toh model ka status "Critical" ki taraf jata hai ya nahi?
print("üß™ Running Sensitivity Audit...")

# Ek dummy range banate hain temperature ki
test_temp_range = np.linspace(df_industrial['Actual_Discharge_T'].min(),
                              df_industrial['Actual_Discharge_T'].max(), 100)

# Baaki sensors ko constant (mean) rakhte hain
sensitivity_df = pd.DataFrame({
    'Suction_P': [df_industrial['Suction_P'].mean()] * 100,
    'Discharge_P': [df_industrial['Discharge_P'].mean()] * 100,
    'Suction_T': [df_industrial['Suction_T'].mean()] * 100,
    'Actual_Discharge_T': test_temp_range,
    'Entropy_Gen': np.linspace(df_industrial['Entropy_Gen'].min(),
                               df_industrial['Entropy_Gen'].max(), 100)
})

# Model se probabilities mangte hain
probs = model.predict_proba(sensitivity_df)

plt.figure(figsize=(10, 5))
for i, class_name in enumerate(le.classes_):
    plt.plot(test_temp_range - 273.15, probs[:, i], label=class_name, lw=2)

plt.axvline(x=75, color='r', linestyle='--', label='Theoretical Threshold') # Example threshold
plt.xlabel("Discharge Temperature (¬∞C)")
plt.ylabel("Prediction Confidence")
plt.title("Phase 4 Audit: Model Sensitivity to Thermal Degradation")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# 2. THE "REAL CHALLENGE": Model without Entropy
# Agar hum Entropy feature hata dein, toh kya accuracy girti hai?
X_no_physics = df_industrial[['Suction_P', 'Discharge_P', 'Suction_T', 'Actual_Discharge_T']]
X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_no_physics, y, test_size=0.2, random_state=42)

model_no_physics = xgb.XGBClassifier(n_estimators=100)
model_no_physics.fit(X_train_np, y_train_np)

print("\n--- ‚ö†Ô∏è Performance WITHOUT Entropy Feature ---")
print(f"Accuracy: {model_no_physics.score(X_test_np, y_test_np):.4f}")

# --- PHASE 3: PHYSICS-INFORMED RESIDUAL AUDIT (FIXED) ---

def physics_informed_audit(input_data, model, label_encoder):
    # 1. Physics Constants
    gamma = 1.4
    Cp = 1.005 # kJ/kg.K

    # 2. Extract Data (Using your specific column names)
    # Mapping Suction_P to P_in and Discharge_P to P_out for calculation
    p_in = input_data['Suction_P']
    p_out = input_data['Discharge_P']
    t_in = input_data['Suction_T']
    t_out_actual = input_data['Actual_Discharge_T']

    # 3. Physics Logic: Ideal T_out (Isentropic)
    # Formula: T2 = T1 * (P2/P1)^((gamma-1)/gamma)
    T_out_ideal = t_in * (p_out / p_in)**((gamma-1)/gamma)

    # Calculate Residual (Thermal Leakage)
    # Yeh batata hai ki kitni energy heat mein waste ho rahi hai
    thermal_residual = t_out_actual - T_out_ideal

    # 4. ML Prediction
    preds = model.predict(input_data)
    probs = model.predict_proba(input_data)

    status_labels = label_encoder.inverse_transform(preds)

    # 5. Build Audit Report
    audit_df = input_data.copy()
    audit_df['Predicted_Status'] = status_labels
    audit_df['Confidence_%'] = np.round(probs.max(axis=1) * 100, 2)
    audit_df['Thermal_Leakage_K'] = np.round(thermal_residual, 2)

    return audit_df

# Testing the Audit on Test Set
# Make sure X_test has 'Suction_P', 'Discharge_P', etc.
audit_results = physics_informed_audit(X_test, model, le)

# Visualizing Phase 3: The "Confidence vs Leakage" Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=audit_results, x='Thermal_Leakage_K', y='Confidence_%',
                hue='Predicted_Status', style='Predicted_Status', s=100, palette='viridis')

plt.title("Phase 3 Audit: Thermal Leakage (Physics) vs. AI Confidence")
plt.xlabel("Thermal Leakage (Actual - Ideal) [Kelvin]")
plt.ylabel("Model Confidence (%)")
plt.axvline(x=0, color='red', linestyle='--', alpha=0.5) # Ideal point
plt.grid(True, alpha=0.3)
plt.show()

print("‚úÖ Audit Results Ready!")
display(audit_results[['Thermal_Leakage_K', 'Predicted_Status', 'Confidence_%']].head())

# --- PHASE 4: CARBON TAX & FINANCIAL AUDIT ---

def financial_carbon_audit(audit_data):
    # Industrial Constants
    CARBON_TAX_RATE = 2500  # INR per Tonne of CO2
    ELECTRICITY_COST = 7.5  # INR per kWh
    T0 = 298.15             # Reference Temp (Kelvin)

    # Column mapping to prevent KeyError
    s_gen_col = 'Net_S_gen' if 'Net_S_gen' in audit_data.columns else 'Entropy_Gen'

    # 1. Power Loss Calculation (Gouy-Stodola)
    # W_lost = T0 * S_gen
    audit_data['Power_Loss_kW'] = T0 * audit_data[s_gen_col]

    # 2. Hourly Financial Loss
    audit_data['Hourly_Loss_INR'] = audit_data['Power_Loss_kW'] * ELECTRICITY_COST

    # 3. Carbon Impact (Assuming 0.8kg CO2 per kWh)
    audit_data['CO2_Wasted_kg_hr'] = audit_data['Power_Loss_kW'] * 0.8
    audit_data['Carbon_Tax_INR_hr'] = (audit_data['CO2_Wasted_kg_hr'] / 1000) * CARBON_TAX_RATE

    # 4. Total Operational Risk
    audit_data['Total_Risk_Cost_INR'] = audit_data['Hourly_Loss_INR'] + audit_data['Carbon_Tax_INR_hr']

    return audit_data

# Run calculations
final_audit_report = financial_carbon_audit(audit_results)

# --- VISUALIZATION ---
# Is baar ensure karna ki niche waali saari lines copy ho rahi hain
fig = px.treemap(
    final_audit_report,
    path=['Predicted_Status'],
    values='Total_Risk_Cost_INR',
    color='Confidence_%',
    title="Phase 4 Audit: Financial & Carbon Risk Exposure",
    color_continuous_scale='RdYlGn_r'
)
fig.show()

# Quick summary for the engineer
avg_loss = final_audit_report[final_audit_report['Predicted_Status'].str.contains('Critical')]['Total_Risk_Cost_INR'].mean()
print(f"\nüí∞ AUDIT SUMMARY:")
print(f"Average Financial Risk for 'Critical' Units: ‚Çπ{avg_loss:.2f} per hour")

import pandas as pd
import numpy as np
import joblib

# --- 1. PREPARE TEST SUITE ---

# A. 10 Test Cases FROM Dataset (Internal Reliability)
# Taking a mix of different statuses from your existing audit_results
internal_tests = audit_results.groupby('Predicted_Status').apply(lambda x: x.sample(min(len(x), 4))).reset_index(drop=True).head(10)

# B. 10 Test Cases WITHOUT Dataset (Edge Cases / Physics Stress Tests)
# We manually create scenarios that test the boundaries of thermodynamics
edge_cases_data = {
    'Suction_P': [1.0, 1.0, 1.0, 1.2, 1.0, 1.0, 1.1, 1.05, 1.0, 1.0],
    'Discharge_P': [3.5, 5.5, 4.0, 1.2, 6.0, 3.5, 4.0, 4.5, 2.0, 5.0],
    'Suction_T': [298, 298, 310, 298, 280, 298, 300, 295, 298, 305],
    'Actual_Discharge_T': [350, 550, 380, 299, 600, 300, 420, 450, 310, 580],
    'Entropy_Gen': [0.05, 0.5, 0.1, 0.001, 0.8, -0.01, 0.2, 0.3, 0.02, 0.6] # Note the -0.01 case!
}
edge_tests = pd.DataFrame(edge_cases_data)

# --- 2. RUN THE AUDIT FUNCTION ---

def run_stress_test(test_df, model, le, test_type="Edge Case"):
    # Ensure columns match training features
    features = ['Suction_P', 'Discharge_P', 'Suction_T', 'Actual_Discharge_T', 'Entropy_Gen']
    X = test_df[features]

    preds = model.predict(X)
    probs = model.predict_proba(X).max(axis=1) * 100
    labels = le.inverse_transform(preds)

    test_df['Final_Prediction'] = labels
    test_df['AI_Confidence'] = probs
    test_df['Test_Source'] = test_type

    # Physics Check: Flag if Entropy is negative (Impossible in physics)
    test_df['Physics_Violation'] = test_df['Entropy_Gen'] < 0

    return test_df

# Execute both suites
results_internal = run_stress_test(internal_tests, model, le, "From Dataset")
results_edge = run_stress_test(edge_tests, model, le, "Outside Dataset")

# Combine for final report
full_test_report = pd.concat([results_internal, results_edge], ignore_index=True)

# --- 3. DISPLAY

import pandas as pd
import numpy as np

# --- 1. PREPARE TEST SUITE ---

# A. 10 Test Cases FROM Dataset (Internal Reliability)
# Mapping previous results back to the feature names the model expects
internal_tests = audit_results.groupby('Predicted_Status', group_keys=False).apply(
    lambda x: x.sample(min(len(x), 4)), include_groups=True
).reset_index(drop=True).head(10)

# Rename columns to match final_model features
column_mapping = {
    'Suction_P': 'P_in',
    'Discharge_P': 'P_out',
    'Suction_T': 'T_in',
    'Actual_Discharge_T': 'T_out_actual',
    'Entropy_Gen': 'Net_S_gen'
}
internal_tests = internal_tests.rename(columns=column_mapping)

# Ensure missing columns (Z_factor, Mass_Flow) exist for internal tests
if 'Z_factor' not in internal_tests.columns: internal_tests['Z_factor'] = 0.98
if 'Mass_Flow' not in internal_tests.columns: internal_tests['Mass_Flow'] = 50.0

# B. 10 Test Cases WITHOUT Dataset (Extreme Edge Cases)
edge_cases_data = {
    'P_in': [1.0, 1.0, 1.0, 1.2, 1.0, 1.0, 1.1, 1.05, 1.0, 1.0],
    'P_out': [3.5, 5.5, 4.0, 1.2, 8.0, 3.5, 4.0, 4.5, 2.0, 5.0],
    'T_in': [298, 298, 310, 298, 280, 298, 300, 295, 298, 305],
    'T_out_actual': [350, 550, 380, 299, 700, 300, 420, 450, 310, 580],
    'Z_factor': [0.98]*10,
    'Mass_Flow': [2.5]*10,
    'Net_S_gen': [0.05, 0.5, 0.1, 0.001, 1.2, -0.05, 0.2, 0.3, 0.02, 0.6]
}
edge_tests = pd.DataFrame(edge_cases_data)

# --- 2. DEFINE THE MASTER AUDIT FUNCTION ---

def run_master_audit(test_df, model, le, test_type="Edge Case"):
    features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Z_factor', 'Mass_Flow', 'Net_S_gen']
    X = test_df[features]

    preds = model.predict(X)
    probs = model.predict_proba(X).max(axis=1) * 100
    labels = le.inverse_transform(preds)

    test_df['Final_Prediction'] = labels
    test_df['AI_Confidence_%'] = np.round(probs, 2)
    test_df['Test_Source'] = test_type

    test_df['Physics_Status'] = test_df['Net_S_gen'].apply(
        lambda x: 'VALID' if x >= 0 else 'INVALID (Negative Entropy Detected!)'
    )
    return test_df

# --- 3. EXECUTION ---

results_internal = run_master_audit(internal_tests, final_model, le, "Internal (Seen Data)")
results_edge = run_master_audit(edge_tests, final_model, le, "External (Stress Test)")

full_audit_report = pd.concat([results_internal, results_edge], ignore_index=True)

# --- 4. DISPLAY RESULTS ---
print("üõ°Ô∏è --- ANTI-GRAVITY INDUSTRIAL AUDIT: 20 POINT CHECK --- üõ°Ô∏è")
display(full_audit_report[['Test_Source', 'Net_S_gen', 'Final_Prediction', 'AI_Confidence_%', 'Physics_Status']])

def safety_integrated_predict(sensor_data, model, le):
    # 1. Define the features the model expects
    features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Z_factor', 'Mass_Flow', 'Net_S_gen']

    # 2. Get raw ML prediction using only feature columns
    X = sensor_data[features]
    ml_pred_idx = model.predict(X)
    ml_probs = model.predict_proba(X).max(axis=1)
    ml_labels = le.inverse_transform(ml_pred_idx)

    final_output = []

    # 3. Apply "Engineering Common Sense" (The Guardrail)
    for i, label in enumerate(ml_labels):
        s_gen = sensor_data.iloc[i]['Net_S_gen']

        if s_gen < 0:
            final_output.append("üö® SENSOR ERROR: Violation of 2nd Law")
        elif s_gen > 0.5 and label == "Healthy":
            final_output.append("‚ö†Ô∏è AUDIT ALERT: High Entropy but AI says Healthy")
        else:
            final_output.append(label)

    return final_output

# Test it on your Edge Cases
safe_predictions = safety_integrated_predict(edge_tests, final_model, le)
print("üõ°Ô∏è Safety-Wrapped Predictions:")
for i, res in enumerate(safe_predictions):
    print(f"Case {i}: {res}")

import pandas as pd
import numpy as np

# --- 1. DATASET A: INTERNAL (WITHIN DATASET) ---
# Hum existing audit_results se random samples uthayenge validation ke liye
internal_benchmark_df = audit_results.sample(10, random_state=42).reset_index(drop=True)

# Mapping columns to match the 'final_model' training features
column_mapping = {
    'Suction_P': 'P_in',
    'Discharge_P': 'P_out',
    'Suction_T': 'T_in',
    'Actual_Discharge_T': 'T_out_actual',
    'Entropy_Gen': 'Net_S_gen'
}
internal_benchmark_df = internal_benchmark_df.rename(columns=column_mapping)

# Ensure missing columns (Z_factor, Mass_Flow) exist for internal tests
if 'Z_factor' not in internal_benchmark_df.columns: internal_benchmark_df['Z_factor'] = 0.98
if 'Mass_Flow' not in internal_benchmark_df.columns: internal_benchmark_df['Mass_Flow'] = 50.0

# --- 2. DATASET B: STRESS-TEST (WITHOUT/OUTSIDE DATASET) ---
edge_cases_data = {
    'P_in': [1.0, 1.0, 1.05, 1.1, 1.0, 1.0, 1.0, 1.0, 1.1, 1.0],
    'P_out': [1.1, 6.5, 4.0, 4.2, 3.8, 3.5, 4.0, 5.0, 3.5, 5.5],
    'T_in': [298, 298, 315, 300, 298, 298, 290, 305, 298, 310],
    'T_out_actual': [300, 650, 450, 410, 350, 300, 420, 580, 350, 600],
    'Z_factor': [0.99, 0.95, 0.98, 0.97, 0.98, 0.98, 0.98, 0.96, 0.98, 0.94],
    'Mass_Flow': [1.0, 5.0, 2.5, 2.5, 2.5, 2.5, 2.5, 4.0, 2.5, 5.5],
    'Net_S_gen': [0.005, 0.85, 0.35, 0.15, 0.08, -0.05, 0.45, 0.65, 0.09, 0.75]
}
stress_test_df = pd.DataFrame(edge_cases_data)

# --- 3. AUDIT EXECUTION ENGINE ---

def run_dual_audit(df, model, le, title):
    print(f"\nüîç RUNNING AUDIT: {title}")

    features = ['P_in', 'P_out', 'T_in', 'T_out_actual', 'Z_factor', 'Mass_Flow', 'Net_S_gen']
    X = df[features]

    preds = model.predict(X)
    probs = model.predict_proba(X).max(axis=1) * 100
    labels = le.inverse_transform(preds)

    df['AI_Decision'] = labels
    df['Confidence_%'] = np.round(probs, 2)

    df['Safety_Verdict'] = df['Net_S_gen'].apply(
        lambda x: '‚úÖ VALID' if x >= 0 else 'üö® INVALID (2nd Law Violation)'
    )

    return df

# Run Audits
internal_report = run_dual_audit(internal_benchmark_df, final_model, le, "INTERNAL BENCHMARK")
stress_report = run_dual_audit(stress_test_df, final_model, le, "EXTERNAL STRESS TEST")

# --- 4. DISPLAY COMPARISON ---
print("\n--- ‚úÖ REPORT 1: INTERNAL SAMPLES ---")
display(internal_report[['Net_S_gen', 'AI_Decision', 'Confidence_%', 'Safety_Verdict']])

print("\n--- üõ°Ô∏è REPORT 2: EXTERNAL STRESS CASES ---")
display(stress_report[['Net_S_gen', 'AI_Decision', 'Confidence_%', 'Safety_Verdict']])